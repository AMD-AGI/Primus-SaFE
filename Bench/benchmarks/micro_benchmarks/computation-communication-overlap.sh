#!/bin/bash

#
# Copyright (c) 2025, Advanced Micro Devices, Inc. All rights reserved.
# See LICENSE for license information.
#

export WORLD_SIZE=${WORLD_SIZE}
export RANK=${RANK}
export MASTER_ADDR=${MASTER_ADDR}
export MASTER_PORT=${MASTER_PORT}
export NCCL_SOCKET_IFNAME=${NCCL_SOCKET_IFNAME:-"eno0"}
export BNIC=${BNIC:-48}
export BXGMI=${BXGMI:-315}
export MAX_RETRY=${MAX_RETRY:-1}
export NCCL_IB_GID_INDEX=${NCCL_IB_GID_INDEX:-3}
export GPUS_PER_NODE=${GPUS_PER_NODE:-8}
export NCCL_TIMEOUT=7200
export TORCH_DISTRIBUTED_DEFAULT_TIMEOUT=$NCCL_TIMEOUT
export GLOO_TIMEOUT=$NCCL_TIMEOUT

NCCL_DEBUG_SUBSYS=ALL
TORCH_NCCL_TRACE_BUFFER_SIZE=10
NCCL_ASYNC_ERROR_HANDLING=1
RDMAV_FORK_SAFE="1"
PYTORCH_HIP_ALLOC_CONF="max_split_size_mb:512"
RCCL_DEBUG=INFO
RCCL_MSCCLPP_ENABLE="0"
TORCH_DISTRIBUTED_DEBUG=DETAIL
NCCL_CHECKS_DISABLE=1
NCCL_IB_GID_INDEX=3
NCCL_CROSS_NIC=0
GPUS_PER_NODE=8
CUDA_DEVICE_MAX_CONNECTIONS=1
NCCL_BLOCKING_WAIT=1
GLOO_SOCKET_IFNAME=eno0
IP_INTERFACE=eno0
NCCL_IB_HCA=rdma0:1,rdma1:1,rdma2:1,rdma3:1,rdma4:1,rdma5:1,rdma6:1,rdma7:1
HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
echo WORLD_SIZE=$WORLD_SIZE RANK=$RANK MASTER_ADDR=$MASTER_ADDR \
    MASTER_PORT=$MASTER_PORT NCCL_SOCKET_IFNAME=$NCCL_SOCKET_IFNAME NCCL_IB_HCA$NCCL_IB_HCA
torchrun --nproc_per_node=$GPUS_PER_NODE \
    --nnodes=$WORLD_SIZE \
    --node_rank=$RANK \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    computation-communication-overlap.py