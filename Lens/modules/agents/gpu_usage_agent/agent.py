"""GPU Usage Analysis Agent - Enhanced with Analysis Features."""

import json
import logging
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime

from langchain_core.messages import SystemMessage
from langchain_core.language_models import BaseChatModel

from .tools import GPUAnalysisTools
from .utils import safe_json_parse
from .prompts import UNDERSTAND_PROMPT

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å¯¼å…¥ç¼“å­˜ç›¸å…³æ¨¡å—
try:
    from cache.base import CacheBase
    from llm_wrapper import CachedLLM
    CACHE_AVAILABLE = True
except ImportError:
    CACHE_AVAILABLE = False
    logger.warning("ç¼“å­˜æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä¸ä½¿ç”¨ LLM ç¼“å­˜")


class GPUUsageAnalysisAgent:
    """GPU ä½¿ç”¨ç‡åˆ†æ Agent - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒæ·±åº¦åˆ†æ"""
    
    def __init__(
        self,
        llm: BaseChatModel,
        api_base_url: str,
        cluster_name: Optional[str] = None,
        cache: Optional[CacheBase] = None,
        cache_enabled: bool = True
    ):
        """
        åˆå§‹åŒ– Agent
        
        Args:
            llm: è¯­è¨€æ¨¡å‹
            api_base_url: Lens API åŸºç¡€ URL
            cluster_name: é›†ç¾¤åç§°ï¼ˆå¯é€‰ï¼‰
            cache: ç¼“å­˜å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        # å¦‚æœå¯ç”¨ç¼“å­˜ä¸”ç¼“å­˜å¯ç”¨ï¼Œä½¿ç”¨ CachedLLM åŒ…è£…
        if cache_enabled and cache is not None and CACHE_AVAILABLE:
            self.llm = CachedLLM(llm, cache=cache, cache_enabled=True)
            self.cache_enabled = True
            logger.info("LLM ç¼“å­˜å·²å¯ç”¨")
        else:
            self.llm = llm
            self.cache_enabled = False
            if cache_enabled and cache is not None:
                logger.warning("ç¼“å­˜æ¨¡å—ä¸å¯ç”¨ï¼Œå°†ä¸ä½¿ç”¨ LLM ç¼“å­˜")
        
        self.api_base_url = api_base_url
        self.cluster_name = cluster_name
        
        # åˆå§‹åŒ–å·¥å…·é›†
        self.tools_manager = GPUAnalysisTools(api_base_url, cluster_name)
    
    def _understand_query(self, user_query: str) -> Dict[str, Any]:
        """ç†è§£ç”¨æˆ·æŸ¥è¯¢ï¼Œè¯†åˆ«éœ€è¦æŸ¥è¯¢çš„ç»´åº¦å’Œå‚æ•°"""
        prompt = UNDERSTAND_PROMPT.format(user_query=user_query)
        messages = [SystemMessage(content=prompt)]
        
        try:
            logger.info(f"æ­£åœ¨ç†è§£ç”¨æˆ·æŸ¥è¯¢: {user_query}")
            response = self.llm.invoke(messages)
            logger.info("æŸ¥è¯¢ç†è§£å®Œæˆ")
            
            # è§£æ LLM è¿”å›çš„ JSON
            result = safe_json_parse(response.content)
            
            if result is None:
                logger.warning(f"æ— æ³•è§£æ LLM è¿”å›çš„ JSON: {response.content[:200]}...")
                return {
                    "needs_clarification": True,
                    "clarification_question": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ï¼Œèƒ½å¦é‡æ–°æè¿°ä¸€ä¸‹ï¼Ÿ",
                    "entities": {}
                }
            
            return result
            
        except Exception as e:
            # è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
            import traceback
            error_type = type(e).__name__
            error_msg = str(e)
            error_traceback = traceback.format_exc()
            
            logger.error("=" * 80)
            logger.error(f"æŸ¥è¯¢ç†è§£å¤±è´¥ - ç”¨æˆ·æŸ¥è¯¢: {user_query}")
            logger.error(f"é”™è¯¯ç±»å‹: {error_type}")
            logger.error(f"é”™è¯¯æ¶ˆæ¯: {error_msg}")
            logger.error(f"å®Œæ•´å †æ ˆè·Ÿè¸ª:\n{error_traceback}")
            logger.error("=" * 80)
            
            return {
                "needs_clarification": True,
                "clarification_question": f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {error_type} - {error_msg}",
                "entities": {},
                "error_details": {
                    "type": error_type,
                    "message": error_msg,
                    "traceback": error_traceback
                }
            }
    
    def _analyze_cluster_trend_with_chart(self, time_range_days: int, granularity: str = "hour") -> Dict[str, Any]:
        """
        åˆ†æclusterçº§åˆ«çš„ä½¿ç”¨ç‡å’Œå ç”¨ç‡è¶‹åŠ¿ï¼Œè¿”å›æŠ˜çº¿å›¾æ•°æ®
        
        Args:
            time_range_days: æ—¶é—´èŒƒå›´ï¼ˆå¤©æ•°ï¼‰
            granularity: æ—¶é—´ç²’åº¦
        
        Returns:
            åŒ…å«æŠ˜çº¿å›¾æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        logger.info("å¼€å§‹åˆ†æé›†ç¾¤è¶‹åŠ¿...")
        
        try:
            # è°ƒç”¨APIè·å–cluster hourly stats
            result = self.tools_manager.query_gpu_usage_trend(
                dimension="cluster",
                granularity=granularity,
                time_range_days=time_range_days,
                metric_type="utilization"
            )
            
            data = safe_json_parse(result)
            if not data or "data_points" not in data:
                return {"error": "æ— æ³•è·å–é›†ç¾¤æ•°æ®"}
            
            data_points = data.get("data_points", [])
            statistics = data.get("statistics", {})
            
            # æ„å»ºæŠ˜çº¿å›¾æ•°æ®ï¼ˆåŒæ—¶åŒ…å«ä½¿ç”¨ç‡å’Œå ç”¨ç‡ï¼‰
            chart_data = {
                "title": "é›†ç¾¤GPUä½¿ç”¨ç‡å’Œå ç”¨ç‡è¶‹åŠ¿",
                "x_axis": [],  # æ—¶é—´è½´
                "series": [
                    {
                        "name": "ä½¿ç”¨ç‡ (Utilization)",
                        "data": [],
                        "type": "line"
                    },
                    {
                        "name": "å ç”¨ç‡ (Allocation Rate)", 
                        "data": [],
                        "type": "line"
                    }
                ]
            }
            
            for dp in data_points:
                timestamp = dp.get("stat_hour", "")
                avg_util = dp.get("avg_utilization", 0) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                alloc_rate = dp.get("allocation_rate", 0) * 100
                
                chart_data["x_axis"].append(timestamp)
                chart_data["series"][0]["data"].append(round(avg_util, 2))
                chart_data["series"][1]["data"].append(round(alloc_rate, 2))
            
            # è®¡ç®—å ç”¨ç‡ç»Ÿè®¡ï¼ˆä» data_points ä¸­æå–ï¼‰
            alloc_rates = [dp.get("allocation_rate", 0) for dp in data_points]
            avg_alloc_rate = sum(alloc_rates) / len(alloc_rates) if alloc_rates else 0
            max_alloc_rate = max(alloc_rates) if alloc_rates else 0
            min_alloc_rate = min(alloc_rates) if alloc_rates else 0
            
            return {
                "chart_data": chart_data,
                "statistics": {
                    "utilization": {
                        "average": round(statistics.get("average", 0) * 100, 2),
                        "max": round(statistics.get("max", 0) * 100, 2),
                        "min": round(statistics.get("min", 0) * 100, 2),
                        "trend": statistics.get("trend", "unknown")
                    },
                    "allocation_rate": {
                        "average": round(avg_alloc_rate * 100, 2),
                        "max": round(max_alloc_rate * 100, 2),
                        "min": round(min_alloc_rate * 100, 2)
                    },
                    "sample_count": statistics.get("sample_count", 0),
                    "time_range_days": time_range_days
                }
            }
            
        except Exception as e:
            logger.error(f"åˆ†æé›†ç¾¤è¶‹åŠ¿å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_namespace_usage(self, time_range_days: int, top_n: int = 10) -> Dict[str, Any]:
        """
        åˆ†ænamespaceçº§åˆ«çš„ä½¿ç”¨ç‡
        
        Args:
            time_range_days: æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰
            top_n: è¿”å›å‰Nä¸ªnamespace
            
        Returns:
            åŒ…å«namespaceåˆ†æç»“æœçš„å­—å…¸
        """
        logger.info("å¼€å§‹åˆ†ænamespaceçº§åˆ«ä½¿ç”¨ç‡...")
        
        try:
            # è·å–æ‰€æœ‰namespaces
            namespaces_result = self.tools_manager.get_available_namespaces(time_range_days)
            namespaces_data = safe_json_parse(namespaces_result)
            namespaces = namespaces_data.get("namespaces", [])
            
            if not namespaces:
                return {"error": "æœªæ‰¾åˆ°namespaceæ•°æ®"}
            
            # è·å–æ¯ä¸ªnamespaceçš„ä½¿ç”¨ç‡æ•°æ®
            namespace_stats = []
            for ns in namespaces[:top_n]:
                try:
                    ns_result = self.tools_manager.query_gpu_usage_trend(
                        dimension="namespace",
                        dimension_value=ns,
                        granularity="hour",
                        time_range_days=time_range_days,
                        metric_type="utilization"
                    )
                    
                    ns_data = safe_json_parse(ns_result)
                    if ns_data and "statistics" in ns_data:
                        stats = ns_data["statistics"]
                        data_points = ns_data.get("data_points", [])
                        
                        # è®¡ç®—å¹³å‡åˆ†é…çš„GPUæ•°é‡
                        avg_gpu_count = 0
                        if data_points:
                            total_gpu = sum(dp.get("allocated_gpu_count", 0) for dp in data_points)
                            avg_gpu_count = total_gpu / len(data_points)
                        
                        namespace_stats.append({
                            "namespace": ns,
                            "avg_utilization": round(stats.get("average", 0) * 100, 2),
                            "max_utilization": round(stats.get("max", 0) * 100, 2),
                            "min_utilization": round(stats.get("min", 0) * 100, 2),
                            "trend": stats.get("trend", "unknown"),
                            "avg_gpu_count": round(avg_gpu_count, 2)
                        })
                except Exception as e:
                    logger.error(f"è·å–namespace {ns} æ•°æ®å¤±è´¥: {str(e)}")
            
            # æŒ‰å¹³å‡ä½¿ç”¨ç‡æ’åº
            namespace_stats.sort(key=lambda x: x["avg_utilization"])
            
            return {
                "namespaces": namespace_stats,
                "total_count": len(namespace_stats),
                "summary": f"åˆ†æäº† {len(namespace_stats)} ä¸ªnamespaces"
            }
            
        except Exception as e:
            logger.error(f"åˆ†ænamespaceä½¿ç”¨ç‡å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _find_low_utilization_annotations(
        self, 
        time_range_days: int,
        top_n_per_key: int = 20  # æ¯ä¸ªkeyè¿”å›top Nä¸ªvalues
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        æ‰¾å‡ºå ç”¨GPUå¤šä½†ä½¿ç”¨ç‡ä½çš„annotations
        
        å¯¹äºæ¯ä¸ªannotation keyï¼Œæ‰¾å‡ºå…¶valuesä¸­å ç”¨GPUæœ€å¤šä½†åˆ©ç”¨ç‡æœ€ä½çš„top N
        
        Args:
            time_range_days: æ—¶é—´èŒƒå›´
            top_n_per_key: æ¯ä¸ªannotation keyè¿”å›çš„top Nä¸ªvalues
            
        Returns:
            (ä½ä½¿ç”¨ç‡annotationåˆ—è¡¨, æ‰€æœ‰annotationæ•°æ®)
        """
        logger.info("å¼€å§‹åˆ†æannotationä½¿ç”¨æƒ…å†µ...")
        
        try:
            # è·å–æ‰€æœ‰annotation keys
            keys_result = self.tools_manager.get_available_dimension_keys("annotation", time_range_days)
            keys_data = safe_json_parse(keys_result)
            annotation_keys = keys_data.get("dimension_keys", [])
            
            if not annotation_keys:
                return [], {"error": "æœªæ‰¾åˆ°annotationæ•°æ®"}
            
            all_results = []
            results_by_key = {}
            
            # å¯¹æ¯ä¸ªannotation keyï¼Œä½¿ç”¨å·¥å…·æ–¹æ³•æ‰¾å‡ºä½ä½¿ç”¨ç‡çš„top N values
            for key in annotation_keys[:10]:  # é™åˆ¶å¤„ç†å‰10ä¸ªkey
                try:
                    logger.info(f"åˆ†æannotation key: {key}")
                    
                    # è°ƒç”¨toolsæ–¹æ³•ï¼Œè·å–è¯¥keyä¸‹top Nçš„values
                    result_str = self.tools_manager.find_low_utilization_dimension_values(
                        dimension_type="annotation",
                        dimension_key=key,
                        time_range_days=time_range_days,
                        top_n=top_n_per_key
                    )
                    
                    result_data = safe_json_parse(result_str)
                    
                    if result_data and "results" in result_data and result_data["results"]:
                        # ä¿å­˜è¯¥keyçš„ç»“æœ
                        results_by_key[key] = result_data
                        
                        # è½¬æ¢æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
                        for item in result_data["results"]:
                            all_results.append({
                                "annotation_key": key,
                                "annotation_value": item["dimension_value"],
                                "avg_utilization": item["avg_utilization"],
                                "avg_gpu_count": item["avg_gpu_count"],
                                "max_utilization": item.get("max_utilization", 0),
                                "min_utilization": item.get("min_utilization", 0),
                                "trend": item.get("trend", "unknown"),
                                "issue_score": item["issue_score"]
                            })
                        
                        logger.info(f"Key {key}: æ‰¾åˆ° {len(result_data['results'])} ä¸ªä½ä½¿ç”¨ç‡values")
                    
                except Exception as e:
                    logger.error(f"åˆ†æannotation key {key} å¤±è´¥: {str(e)}")
            
            # æŒ‰é—®é¢˜è¯„åˆ†å…¨å±€æ’åºï¼ˆåˆ†æ•°è¶Šé«˜è¶Šä¸¥é‡ï¼‰
            all_results.sort(key=lambda x: x["issue_score"], reverse=True)
            
            return all_results, {
                "results_by_key": results_by_key,
                "all_annotations": all_results[:100],  # è¿”å›å‰100ä¸ª
                "total_count": len(all_results),
                "keys_analyzed": len(results_by_key)
            }
            
        except Exception as e:
            logger.error(f"åˆ†æannotationå¤±è´¥: {str(e)}")
            return [], {"error": str(e)}
    
    def _get_workloads_by_annotations(
        self,
        low_util_annotations: List[Dict[str, Any]],
        limit: int = 20
    ) -> Dict[str, Any]:
        """
        æ ¹æ®æ‰¾åˆ°çš„ä½ä½¿ç”¨ç‡annotationsè·å–å¯¹åº”çš„workloadåˆ—è¡¨
        
        Args:
            low_util_annotations: ä½ä½¿ç”¨ç‡annotationåˆ—è¡¨
            limit: æ¯ä¸ªannotationè¿”å›çš„workloadæ•°é‡é™åˆ¶
            
        Returns:
            åŒ…å«workloadè¡¨æ ¼æ•°æ®çš„å­—å…¸
        """
        logger.info("å¼€å§‹æŸ¥è¯¢ä½ä½¿ç”¨ç‡annotationså¯¹åº”çš„workloads...")
        
        if not low_util_annotations:
            return {
                "table_data": [],
                "summary": "æœªæ‰¾åˆ°ä½ä½¿ç”¨ç‡çš„annotations"
            }
        
        try:
            # æ³¨æ„ï¼šLens APIçš„workloadsæ¥å£ç›®å‰ä¸æ”¯æŒç›´æ¥æŒ‰annotationè¿‡æ»¤
            # æˆ‘ä»¬å…ˆè·å–æ‰€æœ‰workloadï¼Œç„¶åæ ¹æ®namespaceç­‰ä¿¡æ¯å…³è”
            # è¿™é‡Œä½œä¸ºç¤ºä¾‹ï¼Œè·å–æœ€è¿‘çš„workloads
            
            workload_table = []
            
            # å¯¹äºæ¯ä¸ªä½ä½¿ç”¨ç‡annotationï¼Œè·å–ç›¸å…³workloads
            for anno in low_util_annotations[:10]:  # é™åˆ¶å‰10ä¸ªannotation
                anno_key = anno["annotation_key"]
                anno_value = anno["annotation_value"]
                
                try:
                    # è·å–workloadsï¼ˆå¯ä»¥æŒ‰å…¶ä»–æ¡ä»¶è¿‡æ»¤ï¼‰
                    # è¿™é‡Œæˆ‘ä»¬è·å–æœ€è¿‘çš„workloadsä½œä¸ºç¤ºä¾‹
                    workloads_result = self.tools_manager.analyze_workload_history(
                        time_range_days=7,
                        namespace=None,
                        limit=limit
                    )
                    
                    workloads_data = safe_json_parse(workloads_result)
                    if workloads_data and "workloads" in workloads_data:
                        workloads = workloads_data["workloads"]
                        
                        # ä¸ºæ¯ä¸ªworkloadæ·»åŠ annotationä¿¡æ¯
                        for wl in workloads[:5]:  # æ¯ä¸ªannotationé™åˆ¶5ä¸ªworkload
                            workload_table.append({
                                "annotation_key": anno_key,
                                "annotation_value": anno_value,
                                "annotation_avg_utilization": anno["avg_utilization"],
                                "annotation_avg_gpu_count": anno["avg_gpu_count"],
                                "workload_name": wl.get("name", ""),
                                "workload_namespace": wl.get("namespace", ""),
                                "workload_kind": wl.get("kind", ""),
                                "workload_status": wl.get("status", ""),
                                "workload_gpu_allocated": wl.get("gpuAllocated", 0),
                                "workload_start_time": wl.get("startAt", 0)
                            })
                            
                except Exception as e:
                    logger.error(f"è·å–annotation {anno_key}:{anno_value} çš„workloadså¤±è´¥: {str(e)}")
            
            return {
                "table_data": workload_table,
                "columns": [
                    "annotation_key",
                    "annotation_value", 
                    "annotation_avg_utilization",
                    "annotation_avg_gpu_count",
                    "workload_name",
                    "workload_namespace",
                    "workload_kind",
                    "workload_status",
                    "workload_gpu_allocated"
                ],
                "total_count": len(workload_table),
                "summary": f"æ‰¾åˆ° {len(workload_table)} ä¸ªç›¸å…³workloads"
            }
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢workloadså¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_all_namespaces(self, time_range_days: int, top_n: int = 10) -> Dict[str, Any]:
        """åˆ†ææ‰€æœ‰namespaceçš„ä½¿ç”¨æƒ…å†µ"""
        logger.info("å¼€å§‹åˆ†ææ‰€æœ‰namespaces...")
        
        try:
            # è·å–æ‰€æœ‰namespaces
            namespaces_result = self.tools_manager.get_available_namespaces(time_range_days)
            namespaces_data = safe_json_parse(namespaces_result)
            namespaces = namespaces_data.get("namespaces", [])
            
            if not namespaces:
                return {"error": "æœªæ‰¾åˆ°namespaceæ•°æ®"}
            
            # è·å–æ¯ä¸ªnamespaceçš„ä½¿ç”¨ç‡æ•°æ®
            namespace_stats = []
            for ns in namespaces[:top_n]:
                try:
                    ns_result = self.tools_manager.query_gpu_usage_trend(
                        dimension="namespace",
                        dimension_value=ns,
                        granularity="hour",
                        time_range_days=time_range_days,
                        metric_type="utilization"
                    )
                    
                    ns_data = safe_json_parse(ns_result)
                    if ns_data and "statistics" in ns_data:
                        stats = ns_data["statistics"]
                        data_points = ns_data.get("data_points", [])
                        
                        # è®¡ç®—å¹³å‡åˆ†é…çš„GPUæ•°é‡
                        avg_gpu_count = 0
                        if data_points:
                            total_gpu = sum(dp.get("allocated_gpu_count", 0) for dp in data_points)
                            avg_gpu_count = total_gpu / len(data_points)
                        
                        namespace_stats.append({
                            "namespace": ns,
                            "avg_utilization": round(stats.get("average", 0) * 100, 2),
                            "max_utilization": round(stats.get("max", 0) * 100, 2),
                            "min_utilization": round(stats.get("min", 0) * 100, 2),
                            "trend": stats.get("trend", "unknown"),
                            "avg_gpu_count": round(avg_gpu_count, 2)
                        })
                except Exception as e:
                    logger.error(f"è·å–namespace {ns} æ•°æ®å¤±è´¥: {str(e)}")
            
            # æŒ‰å¹³å‡ä½¿ç”¨ç‡æ’åº
            namespace_stats.sort(key=lambda x: x["avg_utilization"])
            
            return {
                "namespaces": namespace_stats,
                "total_count": len(namespace_stats)
            }
            
        except Exception as e:
            logger.error(f"åˆ†ænamespaceså¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_specific_namespace(self, namespace: str, time_range_days: int) -> Dict[str, Any]:
        """åˆ†æç‰¹å®šnamespaceçš„ä½¿ç”¨æƒ…å†µ"""
        logger.info(f"å¼€å§‹åˆ†ænamespace: {namespace}...")
        
        try:
            ns_result = self.tools_manager.query_gpu_usage_trend(
                dimension="namespace",
                dimension_value=namespace,
                granularity="hour",
                time_range_days=time_range_days,
                metric_type="utilization"
            )
            
            ns_data = safe_json_parse(ns_result)
            if not ns_data or "statistics" not in ns_data:
                return {"error": f"æ— æ³•è·å–namespace {namespace} çš„æ•°æ®"}
            
            stats = ns_data["statistics"]
            data_points = ns_data.get("data_points", [])
            
            # æ„å»ºæŠ˜çº¿å›¾æ•°æ®
            chart_data = {
                "title": f"Namespace {namespace} GPUä½¿ç”¨ç‡è¶‹åŠ¿",
                "x_axis": [],
                "series": [{
                    "name": "ä½¿ç”¨ç‡",
                    "data": [],
                    "type": "line"
                }]
            }
            
            for dp in data_points:
                timestamp = dp.get("stat_hour", "")
                avg_util = dp.get("avg_utilization", 0) * 100
                
                chart_data["x_axis"].append(timestamp)
                chart_data["series"][0]["data"].append(round(avg_util, 2))
            
            # è®¡ç®—å¹³å‡GPUæ•°é‡
            avg_gpu_count = 0
            if data_points:
                total_gpu = sum(dp.get("allocated_gpu_count", 0) for dp in data_points)
                avg_gpu_count = total_gpu / len(data_points)
            
            return {
                "namespace": namespace,
                "chart_data": chart_data,
                "statistics": {
                    "avg_utilization": round(stats.get("average", 0) * 100, 2),
                    "max_utilization": round(stats.get("max", 0) * 100, 2),
                    "min_utilization": round(stats.get("min", 0) * 100, 2),
                    "trend": stats.get("trend", "unknown"),
                    "avg_gpu_count": round(avg_gpu_count, 2)
                }
            }
            
        except Exception as e:
            logger.error(f"åˆ†ænamespace {namespace} å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_all_users(self, time_range_days: int, top_n: int = 20) -> Dict[str, Any]:
        """åˆ†ææ‰€æœ‰ç”¨æˆ·çš„GPUå ç”¨å’Œä½¿ç”¨ç‡æƒ…å†µ"""
        logger.info("å¼€å§‹åˆ†ææ‰€æœ‰ç”¨æˆ·...")
        
        try:
            result_str = self.tools_manager.analyze_user_gpu_usage(
                time_range_days=time_range_days,
                top_n=top_n
            )
            
            result_data = safe_json_parse(result_str)
            
            if not result_data or "results" not in result_data:
                return {"error": "æ— æ³•è·å–ç”¨æˆ·æ•°æ®"}
            
            users = result_data.get("results", [])
            
            # æ„å»ºè¡¨æ ¼æ•°æ®
            table_data = {
                "columns": ["ç”¨æˆ·å", "å¹³å‡GPUå ç”¨", "å¹³å‡ä½¿ç”¨ç‡(%)", "æœ€å¤§ä½¿ç”¨ç‡(%)", "é—®é¢˜è¯„åˆ†"],
                "rows": []
            }
            
            for user in users:
                table_data["rows"].append([
                    user.get("dimension_value", ""),
                    user.get("avg_gpu_count", 0),
                    user.get("avg_utilization", 0),
                    user.get("max_utilization", 0),
                    user.get("issue_score", 0)
                ])
            
            return {
                "table_data": table_data,
                "users": users,
                "total_count": len(users),
                "summary": result_data.get("summary", "")
            }
            
        except Exception as e:
            logger.error(f"åˆ†æç”¨æˆ·å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_specific_user(self, user_name: str, time_range_days: int) -> Dict[str, Any]:
        """åˆ†æç‰¹å®šç”¨æˆ·çš„GPUå ç”¨æƒ…å†µ"""
        logger.info(f"å¼€å§‹åˆ†æç”¨æˆ·: {user_name}...")
        
        try:
            dimension_value = f"primus-safe.user.name:{user_name}"
            user_result = self.tools_manager.query_gpu_usage_trend(
                dimension="annotation",
                dimension_value=dimension_value,
                granularity="hour",
                time_range_days=time_range_days,
                metric_type="utilization"
            )
            
            user_data = safe_json_parse(user_result)
            if not user_data or "statistics" not in user_data:
                return {"error": f"æ— æ³•è·å–ç”¨æˆ· {user_name} çš„æ•°æ®"}
            
            stats = user_data["statistics"]
            data_points = user_data.get("data_points", [])
            
            # æ„å»ºæŠ˜çº¿å›¾æ•°æ®
            chart_data = {
                "title": f"ç”¨æˆ· {user_name} GPUä½¿ç”¨ç‡è¶‹åŠ¿",
                "x_axis": [],
                "series": [{
                    "name": "ä½¿ç”¨ç‡",
                    "data": [],
                    "type": "line"
                }]
            }
            
            for dp in data_points:
                timestamp = dp.get("stat_hour", "")
                avg_util = dp.get("avg_utilization", 0) * 100
                
                chart_data["x_axis"].append(timestamp)
                chart_data["series"][0]["data"].append(round(avg_util, 2))
            
            # è®¡ç®—å¹³å‡GPUæ•°é‡
            avg_gpu_count = 0
            if data_points:
                total_gpu = sum(dp.get("allocated_gpu_count", 0) for dp in data_points)
                avg_gpu_count = total_gpu / len(data_points)
            
            return {
                "user_name": user_name,
                "chart_data": chart_data,
                "statistics": {
                    "avg_utilization": round(stats.get("average", 0) * 100, 2),
                    "max_utilization": round(stats.get("max", 0) * 100, 2),
                    "min_utilization": round(stats.get("min", 0) * 100, 2),
                    "trend": stats.get("trend", "unknown"),
                    "avg_gpu_count": round(avg_gpu_count, 2)
                }
            }
            
        except Exception as e:
            logger.error(f"åˆ†æç”¨æˆ· {user_name} å¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_low_utilization_resources(self, time_range_days: int) -> Dict[str, Any]:
        """åˆ†æä½ä½¿ç”¨ç‡èµ„æºï¼ˆåŒ…å«æ‰€æœ‰annotationsï¼‰"""
        logger.info("å¼€å§‹åˆ†æä½ä½¿ç”¨ç‡èµ„æº...")
        
        try:
            low_util_annos, all_anno_data = self._find_low_utilization_annotations(time_range_days)
            
            return {
                "low_utilization_annotations": low_util_annos,
                "all_annotations_summary": all_anno_data,
                "total_count": len(low_util_annos)
            }
            
        except Exception as e:
            logger.error(f"åˆ†æä½ä½¿ç”¨ç‡èµ„æºå¤±è´¥: {str(e)}")
            return {"error": str(e)}
    
    def chat(
        self,
        user_query: str,
        conversation_history: Optional[List] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}")
            
            # 1. ç†è§£ç”¨æˆ·æŸ¥è¯¢
            understanding = self._understand_query(user_query)
            
            # 2. å¦‚æœéœ€è¦æ¾„æ¸…ï¼Œç›´æ¥è¿”å›
            if understanding.get("needs_clarification"):
                return {
                    "answer": understanding.get("clarification_question", "è¯·æä¾›æ›´å¤šä¿¡æ¯"),
                    "needs_clarification": True,
                    "data": {},
                    "debug_info": {
                        "understanding": understanding
                    }
                }
            
            # 3. è§£ææŸ¥è¯¢å‚æ•°
            entities = understanding.get("entities", {})
            time_range = entities.get("time_range", {})
            analysis_type = entities.get("analysis_type", "full")
            specific_dimension = entities.get("specific_dimension")
            output_format = entities.get("output_format", "both")
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            time_range_days = 7  # é»˜è®¤7å¤©
            if time_range:
                time_value = time_range.get("value", "7d")
                if isinstance(time_value, str) and time_value.endswith("d"):
                    try:
                        time_range_days = int(time_value[:-1])
                    except:
                        time_range_days = 7
            
            # 4. æ ¹æ®åˆ†æç±»å‹æ‰§è¡Œä¸åŒçš„åˆ†æ
            result = {
                "answer": "",
                "needs_clarification": False,
                "data": {},
                "debug_info": {
                    "understanding": understanding,
                    "time_range_days": time_range_days,
                    "analysis_type": analysis_type
                }
            }
            
            if analysis_type == "cluster_trend":
                # é›†ç¾¤è¶‹åŠ¿åˆ†æï¼ˆå¸¦æŠ˜çº¿å›¾ï¼‰
                logger.info("æ‰§è¡Œé›†ç¾¤è¶‹åŠ¿åˆ†æ...")
                cluster_analysis = self._analyze_cluster_trend_with_chart(time_range_days)
                result["data"]["cluster_trend"] = cluster_analysis
                result["answer"] = self._generate_cluster_trend_summary(cluster_analysis)
                
            elif analysis_type == "namespace_analysis":
                # Namespaceåˆ†æ
                logger.info("æ‰§è¡Œnamespaceåˆ†æ...")
                if specific_dimension and specific_dimension.get("type") == "namespace":
                    # åˆ†æç‰¹å®šnamespace
                    namespace_value = specific_dimension.get("value")
                    namespace_analysis = self._analyze_specific_namespace(namespace_value, time_range_days)
                else:
                    # åˆ†ææ‰€æœ‰namespace
                    namespace_analysis = self._analyze_all_namespaces(time_range_days)
                result["data"]["namespace_analysis"] = namespace_analysis
                result["answer"] = self._generate_namespace_summary(namespace_analysis)
                
            elif analysis_type == "user_analysis":
                # ç”¨æˆ·å ç”¨åˆ†æï¼ˆå¸¦è¡¨æ ¼ï¼‰
                logger.info("æ‰§è¡Œç”¨æˆ·å ç”¨åˆ†æ...")
                if specific_dimension and specific_dimension.get("type") == "user":
                    # åˆ†æç‰¹å®šç”¨æˆ·
                    user_name = specific_dimension.get("value")
                    user_analysis = self._analyze_specific_user(user_name, time_range_days)
                else:
                    # åˆ†ææ‰€æœ‰ç”¨æˆ·
                    user_analysis = self._analyze_all_users(time_range_days)
                result["data"]["user_analysis"] = user_analysis
                result["answer"] = self._generate_user_analysis_summary(user_analysis)
                
            elif analysis_type == "low_utilization":
                # ä½ä½¿ç”¨ç‡èµ„æºè¯†åˆ«
                logger.info("åˆ†æä½ä½¿ç”¨ç‡èµ„æº...")
                low_util_analysis = self._analyze_low_utilization_resources(time_range_days)
                result["data"]["low_utilization"] = low_util_analysis
                result["answer"] = self._generate_low_utilization_summary(low_util_analysis)
                
            else:  # "full" - å®Œæ•´åˆ†æ
                logger.info("æ‰§è¡Œå®Œæ•´åˆ†æ...")
                
                # é›†ç¾¤è¶‹åŠ¿
                cluster_analysis = self._analyze_cluster_trend_with_chart(time_range_days)
                result["data"]["cluster_trend"] = cluster_analysis
                
                # Namespaceåˆ†æ
                namespace_analysis = self._analyze_all_namespaces(time_range_days, top_n=10)
                result["data"]["namespace_analysis"] = namespace_analysis
                
                # ç”¨æˆ·åˆ†æ
                user_analysis = self._analyze_all_users(time_range_days, top_n=20)
                result["data"]["user_analysis"] = user_analysis
                
                # ç”Ÿæˆç»¼åˆæ‘˜è¦
                result["answer"] = self._generate_full_analysis_summary(
                    cluster_analysis, namespace_analysis, user_analysis
                )
            
            logger.info("æŸ¥è¯¢å¤„ç†å®Œæˆ")
            return result
        
        except Exception as e:
            logger.error(f"å¤„ç†æŸ¥è¯¢å¤±è´¥: {str(e)}")
            import traceback
            return {
                "answer": f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "needs_clarification": False,
                "data": {},
                "debug_info": {
                    "error": str(e),
                    "traceback": traceback.format_exc()
                }
            }
    
    async def achat(
        self,
        user_query: str,
        conversation_history: Optional[List] = None
    ) -> Dict[str, Any]:
        """
        å¼‚æ­¥å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        # ç®€åŒ–ç‰ˆæœ¬æš‚æ—¶ç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•
        return self.chat(user_query, conversation_history)
    
    async def stream_chat(
        self,
        user_query: str,
        conversation_history: Optional[List] = None
    ):
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œé€æ­¥è¿”å›åˆ†æç»“æœ
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
        
        Yields:
            åŒ…å«åˆ†æè¿›åº¦å’Œç»“æœçš„å­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹æµå¼å¤„ç†æŸ¥è¯¢: {user_query}")
            
            # 1. ç†è§£ç”¨æˆ·æŸ¥è¯¢
            yield {
                "type": "status",
                "stage": "understanding",
                "message": "æ­£åœ¨ç†è§£æ‚¨çš„æŸ¥è¯¢..."
            }
            
            understanding = self._understand_query(user_query)
            
            # 2. å¦‚æœéœ€è¦æ¾„æ¸…ï¼Œç›´æ¥è¿”å›
            if understanding.get("needs_clarification"):
                yield {
                    "type": "final",
                    "answer": understanding.get("clarification_question", "è¯·æä¾›æ›´å¤šä¿¡æ¯"),
                    "needs_clarification": True,
                    "data": {},
                    "debug_info": {
                        "understanding": understanding
                    }
                }
                return
            
            # 3. è§£ææŸ¥è¯¢å‚æ•°
            entities = understanding.get("entities", {})
            time_range = entities.get("time_range", {})
            analysis_type = entities.get("analysis_type", "full")
            specific_dimension = entities.get("specific_dimension")
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            time_range_days = 7  # é»˜è®¤7å¤©
            if time_range:
                time_value = time_range.get("value", "7d")
                if isinstance(time_value, str) and time_value.endswith("d"):
                    try:
                        time_range_days = int(time_value[:-1])
                    except:
                        time_range_days = 7
            
            yield {
                "type": "status",
                "stage": "understanding_complete",
                "message": f"æŸ¥è¯¢ç†è§£å®Œæˆï¼Œåˆ†æç±»å‹: {analysis_type}ï¼Œæ—¶é—´èŒƒå›´: {time_range_days}å¤©"
            }
            
            # 4. æ‰§è¡Œåˆ†æ
            result = {
                "answer": "",
                "needs_clarification": False,
                "data": {},
                "debug_info": {
                    "understanding": understanding,
                    "time_range_days": time_range_days,
                    "analysis_type": analysis_type
                }
            }
            
            if analysis_type == "cluster_trend":
                # é›†ç¾¤è¶‹åŠ¿åˆ†æ
                yield {
                    "type": "status",
                    "stage": "cluster_analysis",
                    "message": "æ­£åœ¨åˆ†æé›†ç¾¤è¶‹åŠ¿..."
                }
                
                cluster_analysis = self._analyze_cluster_trend_with_chart(time_range_days)
                result["data"]["cluster_trend"] = cluster_analysis
                
                yield {
                    "type": "data",
                    "stage": "cluster_analysis_complete",
                    "message": "é›†ç¾¤è¶‹åŠ¿åˆ†æå®Œæˆ",
                    "data": {"cluster_trend": cluster_analysis}
                }
                
                result["answer"] = self._generate_cluster_trend_summary(cluster_analysis)
                
            elif analysis_type == "namespace_analysis":
                # Namespaceåˆ†æ
                yield {
                    "type": "status",
                    "stage": "namespace_analysis",
                    "message": "æ­£åœ¨åˆ†ænamespace..."
                }
                
                if specific_dimension and specific_dimension.get("type") == "namespace":
                    namespace_value = specific_dimension.get("value")
                    namespace_analysis = self._analyze_specific_namespace(namespace_value, time_range_days)
                else:
                    namespace_analysis = self._analyze_all_namespaces(time_range_days)
                
                result["data"]["namespace_analysis"] = namespace_analysis
                
                yield {
                    "type": "data",
                    "stage": "namespace_analysis_complete",
                    "message": "Namespaceåˆ†æå®Œæˆ",
                    "data": {"namespace_analysis": namespace_analysis}
                }
                
                result["answer"] = self._generate_namespace_summary(namespace_analysis)
                
            elif analysis_type == "user_analysis":
                # ç”¨æˆ·åˆ†æ
                yield {
                    "type": "status",
                    "stage": "user_analysis",
                    "message": "æ­£åœ¨åˆ†æç”¨æˆ·å ç”¨æƒ…å†µ..."
                }
                
                if specific_dimension and specific_dimension.get("type") == "user":
                    user_name = specific_dimension.get("value")
                    user_analysis = self._analyze_specific_user(user_name, time_range_days)
                else:
                    user_analysis = self._analyze_all_users(time_range_days)
                
                result["data"]["user_analysis"] = user_analysis
                
                yield {
                    "type": "data",
                    "stage": "user_analysis_complete",
                    "message": "ç”¨æˆ·åˆ†æå®Œæˆ",
                    "data": {"user_analysis": user_analysis}
                }
                
                result["answer"] = self._generate_user_analysis_summary(user_analysis)
                
            elif analysis_type == "low_utilization":
                # ä½ä½¿ç”¨ç‡èµ„æºåˆ†æ
                yield {
                    "type": "status",
                    "stage": "low_utilization_analysis",
                    "message": "æ­£åœ¨åˆ†æä½ä½¿ç”¨ç‡èµ„æº..."
                }
                
                low_util_analysis = self._analyze_low_utilization_resources(time_range_days)
                result["data"]["low_utilization"] = low_util_analysis
                
                yield {
                    "type": "data",
                    "stage": "low_utilization_complete",
                    "message": "ä½ä½¿ç”¨ç‡èµ„æºåˆ†æå®Œæˆ",
                    "data": {"low_utilization": low_util_analysis}
                }
                
                result["answer"] = self._generate_low_utilization_summary(low_util_analysis)
                
            else:  # "full" - å®Œæ•´åˆ†æ
                # é›†ç¾¤è¶‹åŠ¿
                yield {
                    "type": "status",
                    "stage": "cluster_analysis",
                    "message": "æ­£åœ¨åˆ†æé›†ç¾¤è¶‹åŠ¿..."
                }
                
                cluster_analysis = self._analyze_cluster_trend_with_chart(time_range_days)
                result["data"]["cluster_trend"] = cluster_analysis
                
                yield {
                    "type": "data",
                    "stage": "cluster_complete",
                    "message": "é›†ç¾¤åˆ†æå®Œæˆ",
                    "data": {"cluster_trend": cluster_analysis}
                }
                
                # Namespaceåˆ†æ
                yield {
                    "type": "status",
                    "stage": "namespace_analysis",
                    "message": "æ­£åœ¨åˆ†ænamespaces..."
                }
                
                namespace_analysis = self._analyze_all_namespaces(time_range_days, top_n=10)
                result["data"]["namespace_analysis"] = namespace_analysis
                
                yield {
                    "type": "data",
                    "stage": "namespace_complete",
                    "message": "Namespaceåˆ†æå®Œæˆ",
                    "data": {"namespace_analysis": namespace_analysis}
                }
                
                # ç”¨æˆ·åˆ†æ
                yield {
                    "type": "status",
                    "stage": "user_analysis",
                    "message": "æ­£åœ¨åˆ†æç”¨æˆ·å ç”¨æƒ…å†µ..."
                }
                
                user_analysis = self._analyze_all_users(time_range_days, top_n=20)
                result["data"]["user_analysis"] = user_analysis
                
                yield {
                    "type": "data",
                    "stage": "user_complete",
                    "message": "ç”¨æˆ·åˆ†æå®Œæˆ",
                    "data": {"user_analysis": user_analysis}
                }
                
                # ç”Ÿæˆç»¼åˆæ‘˜è¦
                result["answer"] = self._generate_full_analysis_summary(
                    cluster_analysis, namespace_analysis, user_analysis
                )
            
            # è¿”å›æœ€ç»ˆç»“æœ
            yield {
                "type": "final",
                "answer": result["answer"],
                "needs_clarification": False,
                "data": result["data"],
                "debug_info": result["debug_info"]
            }
            
            logger.info("æµå¼æŸ¥è¯¢å¤„ç†å®Œæˆ")
        
        except Exception as e:
            logger.error(f"æµå¼å¤„ç†æŸ¥è¯¢å¤±è´¥: {str(e)}")
            import traceback
            yield {
                "type": "error",
                "answer": f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "needs_clarification": False,
                "data": {},
                "debug_info": {
                    "error": str(e),
                    "traceback": traceback.format_exc()
                }
            }
    
    # ==================== æ‘˜è¦ç”Ÿæˆæ–¹æ³• ====================
    
    def _generate_cluster_trend_summary(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆé›†ç¾¤è¶‹åŠ¿åˆ†ææ‘˜è¦"""
        if "error" in analysis:
            return f"åˆ†æå¤±è´¥: {analysis['error']}"
        
        stats = analysis.get("statistics", {})
        util_stats = stats.get("utilization", {})
        alloc_stats = stats.get("allocation_rate", {})
        
        summary = f"""## é›†ç¾¤GPUä½¿ç”¨æƒ…å†µåˆ†æ

### ğŸ“Š ä½¿ç”¨ç‡ç»Ÿè®¡
- å¹³å‡ä½¿ç”¨ç‡: {util_stats.get('average', 0)}%
- æœ€å¤§ä½¿ç”¨ç‡: {util_stats.get('max', 0)}%
- æœ€å°ä½¿ç”¨ç‡: {util_stats.get('min', 0)}%
- è¶‹åŠ¿: {util_stats.get('trend', 'unknown')}

### ğŸ“ˆ å ç”¨ç‡ç»Ÿè®¡
- å¹³å‡å ç”¨ç‡: {alloc_stats.get('average', 0)}%
- æœ€å¤§å ç”¨ç‡: {alloc_stats.get('max', 0)}%
- æœ€å°å ç”¨ç‡: {alloc_stats.get('min', 0)}%

ğŸ“Œ å·²ç”ŸæˆæŠ˜çº¿å›¾ï¼Œè¯·æŸ¥çœ‹å¯è§†åŒ–ç»“æœã€‚
"""
        return summary
    
    def _generate_namespace_summary(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆnamespaceåˆ†ææ‘˜è¦"""
        if "error" in analysis:
            return f"åˆ†æå¤±è´¥: {analysis['error']}"
        
        # å¦‚æœæ˜¯å•ä¸ªnamespaceåˆ†æ
        if "namespace" in analysis:
            ns = analysis["namespace"]
            stats = analysis.get("statistics", {})
            return f"""## Namespace {ns} GPUä½¿ç”¨æƒ…å†µ

### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
- å¹³å‡ä½¿ç”¨ç‡: {stats.get('avg_utilization', 0)}%
- æœ€å¤§ä½¿ç”¨ç‡: {stats.get('max_utilization', 0)}%
- æœ€å°ä½¿ç”¨ç‡: {stats.get('min_utilization', 0)}%
- å¹³å‡GPUå ç”¨: {stats.get('avg_gpu_count', 0)} å¼ 
- è¶‹åŠ¿: {stats.get('trend', 'unknown')}

ğŸ“Œ å·²ç”ŸæˆæŠ˜çº¿å›¾ï¼Œè¯·æŸ¥çœ‹å¯è§†åŒ–ç»“æœã€‚
"""
        
        # å¦‚æœæ˜¯æ‰€æœ‰namespaceåˆ†æ
        namespaces = analysis.get("namespaces", [])
        total = analysis.get("total_count", 0)
        
        if not namespaces:
            return "æœªæ‰¾åˆ°namespaceæ•°æ®ã€‚"
        
        summary = f"""## Namespace GPUä½¿ç”¨æƒ…å†µåˆ†æ

å…±åˆ†æäº† {total} ä¸ªnamespacesã€‚

### ä½¿ç”¨ç‡æœ€ä½çš„å‰5ä¸ªNamespacesï¼š
"""
        for i, ns in enumerate(namespaces[:5]):
            summary += f"{i+1}. **{ns['namespace']}**: å¹³å‡ä½¿ç”¨ç‡ {ns['avg_utilization']}%, å¹³å‡å ç”¨ {ns['avg_gpu_count']} å¼ GPU\n"
        
        return summary
    
    def _generate_user_analysis_summary(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆç”¨æˆ·åˆ†ææ‘˜è¦"""
        if "error" in analysis:
            return f"åˆ†æå¤±è´¥: {analysis['error']}"
        
        # å¦‚æœæ˜¯å•ä¸ªç”¨æˆ·åˆ†æ
        if "user_name" in analysis:
            user = analysis["user_name"]
            stats = analysis.get("statistics", {})
            return f"""## ç”¨æˆ· {user} GPUä½¿ç”¨æƒ…å†µ

### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
- å¹³å‡ä½¿ç”¨ç‡: {stats.get('avg_utilization', 0)}%
- æœ€å¤§ä½¿ç”¨ç‡: {stats.get('max_utilization', 0)}%
- æœ€å°ä½¿ç”¨ç‡: {stats.get('min_utilization', 0)}%
- å¹³å‡GPUå ç”¨: {stats.get('avg_gpu_count', 0)} å¼ 
- è¶‹åŠ¿: {stats.get('trend', 'unknown')}

ğŸ“Œ å·²ç”ŸæˆæŠ˜çº¿å›¾ï¼Œè¯·æŸ¥çœ‹å¯è§†åŒ–ç»“æœã€‚
"""
        
        # å¦‚æœæ˜¯æ‰€æœ‰ç”¨æˆ·åˆ†æ
        users = analysis.get("users", [])
        total = analysis.get("total_count", 0)
        
        if not users:
            return "æœªæ‰¾åˆ°ç”¨æˆ·æ•°æ®ã€‚"
        
        summary = f"""## ç”¨æˆ·GPUå ç”¨åˆ†æ

å…±åˆ†æäº† {total} ä¸ªç”¨æˆ·ã€‚

### ğŸ” å ç”¨GPUå¤šä½†ä½¿ç”¨ç‡ä½çš„ç”¨æˆ·ï¼ˆæŒ‰é—®é¢˜è¯„åˆ†æ’åºï¼‰ï¼š

| ç”¨æˆ·å | å¹³å‡GPUå ç”¨ | å¹³å‡ä½¿ç”¨ç‡ | æœ€å¤§ä½¿ç”¨ç‡ | é—®é¢˜è¯„åˆ† |
|--------|-------------|------------|------------|----------|
"""
        for user in users[:10]:
            summary += f"| {user['dimension_value']} | {user['avg_gpu_count']} | {user['avg_utilization']}% | {user['max_utilization']}% | {user['issue_score']} |\n"
        
        summary += "\nğŸ’¡ **å»ºè®®**: é—®é¢˜è¯„åˆ†é«˜çš„ç”¨æˆ·å»ºè®®ä¼˜åŒ–GPUä½¿ç”¨æ•ˆç‡æˆ–å‡å°‘å ç”¨ã€‚\n\nğŸ“Š è¯¦ç»†æ•°æ®è§ä¸‹æ–¹è¡¨æ ¼ã€‚"
        
        return summary
    
    def _generate_low_utilization_summary(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆä½ä½¿ç”¨ç‡èµ„æºåˆ†ææ‘˜è¦"""
        if "error" in analysis:
            return f"åˆ†æå¤±è´¥: {analysis['error']}"
        
        low_util_annos = analysis.get("low_utilization_annotations", [])
        total = analysis.get("total_count", 0)
        
        if not low_util_annos:
            return "âœ… æœªå‘ç°æ˜æ˜¾çš„ä½ä½¿ç”¨ç‡èµ„æºé—®é¢˜ã€‚"
        
        summary = f"""## ä½ä½¿ç”¨ç‡èµ„æºåˆ†æ

å‘ç° {total} ä¸ªå ç”¨GPUå¤šä½†ä½¿ç”¨ç‡ä½çš„èµ„æºã€‚

### ğŸ”´ é—®é¢˜æœ€ä¸¥é‡çš„å‰10ä¸ªï¼š

"""
        for i, anno in enumerate(low_util_annos[:10]):
            summary += f"{i+1}. **{anno['annotation_key']}={anno['annotation_value']}**\n"
            summary += f"   - å¹³å‡GPUå ç”¨: {anno['avg_gpu_count']} å¼ \n"
            summary += f"   - å¹³å‡ä½¿ç”¨ç‡: {anno['avg_utilization']}%\n"
            summary += f"   - é—®é¢˜è¯„åˆ†: {anno['issue_score']}\n\n"
        
        summary += "ğŸ’¡ **å»ºè®®**: è”ç³»ç›¸å…³èµ„æºè´Ÿè´£äººï¼Œä¼˜åŒ–GPUä½¿ç”¨æ•ˆç‡ã€‚"
        
        return summary
    
    def _generate_full_analysis_summary(
        self,
        cluster_analysis: Dict[str, Any],
        namespace_analysis: Dict[str, Any],
        user_analysis: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆå®Œæ•´åˆ†ææ‘˜è¦"""
        summary = "# GPUä½¿ç”¨æƒ…å†µå®Œæ•´åˆ†ææŠ¥å‘Š\n\n"
        
        # é›†ç¾¤çº§åˆ«æ‘˜è¦
        summary += "## 1. é›†ç¾¤æ•´ä½“æƒ…å†µ\n\n"
        if "error" not in cluster_analysis:
            stats = cluster_analysis.get("statistics", {})
            util_stats = stats.get("utilization", {})
            alloc_stats = stats.get("allocation_rate", {})
            summary += f"- å¹³å‡ä½¿ç”¨ç‡: {util_stats.get('average', 0)}%\n"
            summary += f"- å¹³å‡å ç”¨ç‡: {alloc_stats.get('average', 0)}%\n"
            summary += f"- è¶‹åŠ¿: {util_stats.get('trend', 'unknown')}\n\n"
            summary += "ğŸ“Š å·²ç”Ÿæˆé›†ç¾¤è¶‹åŠ¿æŠ˜çº¿å›¾ã€‚\n\n"
        else:
            summary += f"é›†ç¾¤åˆ†æå¤±è´¥: {cluster_analysis['error']}\n\n"
        
        # Namespaceçº§åˆ«æ‘˜è¦
        summary += "## 2. Namespaceåˆ†æ\n\n"
        if "error" not in namespace_analysis:
            namespaces = namespace_analysis.get("namespaces", [])
            total_ns = namespace_analysis.get("total_count", 0)
            summary += f"å…±åˆ†æäº† {total_ns} ä¸ªnamespacesã€‚\n\n"
            if namespaces:
                summary += "ä½¿ç”¨ç‡æœ€ä½çš„3ä¸ªnamespaces:\n"
                for i, ns in enumerate(namespaces[:3]):
                    summary += f"{i+1}. {ns['namespace']}: {ns['avg_utilization']}% (å ç”¨{ns['avg_gpu_count']}å¼ GPU)\n"
        else:
            summary += f"Namespaceåˆ†æå¤±è´¥: {namespace_analysis['error']}\n\n"
        
        # ç”¨æˆ·çº§åˆ«æ‘˜è¦
        summary += "\n## 3. ç”¨æˆ·å ç”¨åˆ†æ\n\n"
        if "error" not in user_analysis:
            users = user_analysis.get("users", [])
            total_users = user_analysis.get("total_count", 0)
            summary += f"å…±åˆ†æäº† {total_users} ä¸ªç”¨æˆ·ã€‚\n\n"
            if users:
                summary += "å ç”¨å¤šä½†ä½¿ç”¨ç‡ä½çš„å‰5ä¸ªç”¨æˆ·:\n\n"
                summary += "| ç”¨æˆ·å | å¹³å‡GPUå ç”¨ | å¹³å‡ä½¿ç”¨ç‡ | é—®é¢˜è¯„åˆ† |\n"
                summary += "|--------|-------------|------------|----------|\n"
                for user in users[:5]:
                    summary += f"| {user['dimension_value']} | {user['avg_gpu_count']} | {user['avg_utilization']}% | {user['issue_score']} |\n"
                summary += "\nğŸ“Š è¯¦ç»†ç”¨æˆ·æ•°æ®è§è¡¨æ ¼ã€‚\n"
        else:
            summary += f"ç”¨æˆ·åˆ†æå¤±è´¥: {user_analysis['error']}\n\n"
        
        summary += "\n---\n\nğŸ’¡ **æ€»ä½“å»ºè®®**: é‡ç‚¹å…³æ³¨ä½¿ç”¨ç‡ä½ä½†å ç”¨å¤šçš„ç”¨æˆ·å’Œnamespaceï¼Œä¼˜åŒ–èµ„æºåˆ©ç”¨æ•ˆç‡ã€‚"
        
        return summary